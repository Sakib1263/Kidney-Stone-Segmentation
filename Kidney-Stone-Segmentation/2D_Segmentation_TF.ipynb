{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "sFyb0y7PUJOo"
   },
   "source": [
    "# 2D Kidney and Stone Segmentation Tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "W9vTr2NhcGAA"
   },
   "source": [
    "# Check GPU Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3630,
     "status": "ok",
     "timestamp": 1663560942779,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "VUNJNtNxcFTF",
    "outputId": "4c5607ab-14e8-43a4-a149-11baea7994e0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Is CUDA enabled GPU Available?\", torch.cuda.is_available())\n",
    "print(\"GPU Number:\", torch.cuda.device_count())\n",
    "print(\"Current GPU Index:\", torch.cuda.current_device())\n",
    "print(\"GPU Type:\", torch.cuda.get_device_name(device=None))\n",
    "print(\"GPU Capability:\", torch.cuda.get_device_capability(device=None))\n",
    "print(\"Is GPU Initialized yet?\", torch.cuda.is_initialized())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgW7r0C9TuZk"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4053,
     "status": "ok",
     "timestamp": 1663561042708,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "eMhBhz1CrMb3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import h5py\n",
    "import scipy\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import PIL.Image\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from scipy import interp\n",
    "import albumentations as A\n",
    "from itertools import cycle\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4xmE2dlB1of"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsbdDDiQY-51"
   },
   "source": [
    "## Import and Preprocess Data\n",
    "\n",
    "General Instructions:\n",
    "\n",
    "*   If your Data is in a ZIP/RAR/TAR File, start from Step 1. If no preprocessing is required, skip all steps and directly go for model building.\n",
    "*   Step 1 does not require \"Data Configurations\".\n",
    "*   If your data is raw or requires some preprocessing before fold creation, move to Step 2.\n",
    "*   If you require to create folds, go to Step 3, skip otherwise.\n",
    "*   If you require to Augment Images, mainly in the Training Set, follow Step 4.\n",
    "*   Visualize any image from any fold from the Train/Test/Validation set.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMJpiCRRB1oh"
   },
   "source": [
    "### Step 1: Import and UnZip/UnRaR/UnTar Files based on the Name and File-type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27484,
     "status": "ok",
     "timestamp": 1663561104747,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "P0dZPT7sD08H",
    "outputId": "01e4d9f8-696b-40a0-e0ec-716292ee1432"
   },
   "outputs": [],
   "source": [
    "!gdown --id 1GtmdmZaYcJGAUgRKC5gGw73nuqoo0k3c # Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jLx1qkRRSjKy"
   },
   "source": [
    "UnZIP for ZIP Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mJv_HC3ISiW"
   },
   "outputs": [],
   "source": [
    "!jar xvf Data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro8YYvE8ZVDX"
   },
   "source": [
    "UnRAR for RAR Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gncEdB9UZR4n"
   },
   "outputs": [],
   "source": [
    "# !unrar x \"input_file_name.rar\" \"/content/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOff0yjQZW7A"
   },
   "source": [
    "UnTAR for TAR Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxrP_jrSZXAv"
   },
   "outputs": [],
   "source": [
    "# !tar -xvf \"input_file_name.tar\" -C \"/content/\"  #[run this cell to extract tar files]\n",
    "# !tar -xzvf \"input_file_name.tar.gz\" -C \"/content/\"  #[run this cell to extract tar.gz files]\n",
    "# !tar -xjvf \"input_file_name.tar.bz2\" -C \"/content/\"  #[run this cell to extract tar.bz2 files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-3effOA-_Jpl"
   },
   "source": [
    "### Step 2: Preprocess Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vs-PlZD_iGQ5"
   },
   "source": [
    "If any \"Module not Found' error, Upgrade 'albumentations' library to the latest version and restart runtime  \n",
    "[Augmentation List from 'albumentations' GitHub](https://github.com/albumentations-team/albumentations#benchmarking-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqOknW_WQl6B"
   },
   "outputs": [],
   "source": [
    "# !pip install albumentations --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4FExP7oPNSM"
   },
   "source": [
    "Mention the Data Path for Raw Data and provide with a list of processing or augmentation steps, a sample is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 468,
     "status": "ok",
     "timestamp": 1663561780514,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "rmD8TPjnH6VH",
    "outputId": "8626f455-3fb0-4e44-fe03-a9a2e7206778"
   },
   "outputs": [],
   "source": [
    "config_file = configparser.ConfigParser()  # CREATE OBJECT\n",
    "config_file.read(\"Train Configurations.ini\")  # READ THE CONFIG FILE\n",
    "\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"DATA\"]={\n",
    "        \"raw_data_path\": 'Data/Test',  # Directory containing Raw Data\n",
    "        }\n",
    "\n",
    "# SAVE THE SETTINGS TO THE FILE\n",
    "with open(\"Train Configurations.ini\",\"w\") as file_object:\n",
    "    config_file.write(file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1663561780979,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "AKQinTmXIAfV",
    "outputId": "ff5633da-802f-46c8-f981-c2d017d2c702"
   },
   "outputs": [],
   "source": [
    "# PRINT FILE CONTENT\n",
    "read_file = open(\"Train Configurations.ini\", \"r\")\n",
    "content = read_file.read()\n",
    "print(\"Content of the config file are:\\n\")\n",
    "print(content)\n",
    "read_file.flush()\n",
    "read_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1663561782402,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "buj2kDVW_MSR",
    "outputId": "eea26494-6554-4bf2-807a-a42d8e0b946e"
   },
   "outputs": [],
   "source": [
    "'Configurations'\n",
    "config_file = configparser.ConfigParser()  # CREATE OBJECT\n",
    "config_file.read(\"Train Configurations.ini\")  # READ THE CONFIG FILE\n",
    "raw_data_path = config_file[\"DATA\"][\"raw_data_path\"]  # EXTRACT VALUE\n",
    "\n",
    "'List of Processes to be applied per image, vary it according to the requirements'\n",
    "process_list = [A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=1.0),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1663561783300,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "W7PqRpbQIZNG"
   },
   "outputs": [],
   "source": [
    "def preprocess_rawdata(raw_data_path, process_list):\n",
    "    num_folds = len(os.listdir(raw_data_path))\n",
    "    for i in range(1, num_folds + 1):\n",
    "        print(f'\\nCurrently Processing Fold {i}')\n",
    "        # List containing all images of a certain class in a certain fold\n",
    "        Image_List = sorted(os.listdir(f'{raw_data_path}/Fold_{i}/Images/'))\n",
    "        transform = A.Compose(process_list)\n",
    "        for ii in range(0, (len(Image_List))):\n",
    "            current_image = Image_List[ii]\n",
    "            current_image = current_image[0:-4]\n",
    "            # Read an image with OpenCV and convert it to the RGB colorspace\n",
    "            org_image = cv2.imread(f'{raw_data_path}/Fold_{i}/Images/{Image_List[ii]}')  # Read Original Image\n",
    "            img_nparray = np.asarray(org_image)\n",
    "            if img_nparray.shape[2] > 3:\n",
    "                org_image = org_image[:,:,:3]  # Remove Alpha (4th) Channel from the Image\n",
    "            org_image = cv2.cvtColor(org_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB Colorspace\n",
    "            transformed_GT = transform(image=org_image)  # Augment Image and Corresponding Mask\n",
    "            transformed_image = transformed_GT['image']\n",
    "            cv2.imwrite(f'{raw_data_path}/Fold_{i}/Images/{current_image}.png', transformed_image)\n",
    "            print(f'Image Number: {ii+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNVooK9V_MVe"
   },
   "outputs": [],
   "source": [
    "# from Helper_Functions import preprocess_rawdata\n",
    "preprocess_rawdata(raw_data_path, process_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j-9ihhfC8xv"
   },
   "source": [
    "### Step 3: Create Folds from Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2u2BQNNy-6a"
   },
   "source": [
    "Create 'n' Folds from the Raw Dataset, this code only creates random stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49g8WmxXtg-U"
   },
   "outputs": [],
   "source": [
    "'Configurations'\n",
    "config_file = configparser.ConfigParser()  # CREATE OBJECT \n",
    "config_file.read(\"Train Configurations.ini\")  # READ THE CONFIG FILE\n",
    "raw_data_path = config_file[\"DATA\"][\"raw_data_path\"]  # Raw Data Directory, change the name accordingly\n",
    "num_folds = config_file[\"DATA\"][\"num_folds\"]  # Number of Folds required\n",
    "train_portion = config_file[\"DATA\"][\"train_portion\"]  # % of Data to be put in the training set, rest in the test set\n",
    "val_portion = config_file[\"DATA\"][\"val_portion\"]  # % of Train Data to be used for validation; optional parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521215,
     "status": "ok",
     "timestamp": 1640385034711,
     "user": {
      "displayName": "Sakib Mahmud",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "dRT6IXa_jofo",
    "outputId": "86f51f35-5a8b-45b4-d5b7-ae6b58d6dc61"
   },
   "outputs": [],
   "source": [
    "create_folds(raw_data_path, num_folds, train_portion, validation_portion=val_portion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sRdWB241j0RU"
   },
   "source": [
    "### Step 4: Augment Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6nMR00NtDRO"
   },
   "source": [
    "If any \"Module not Found' error or ValueError occurs, upgrade 'albumentations' library to the latest version and restart runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5EGycLJQ37C"
   },
   "outputs": [],
   "source": [
    "!pip install albumentations --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adp0two2tGYk"
   },
   "source": [
    "[Augmentation List from 'albumentations' GitHub](https://github.com/albumentations-team/albumentations#benchmarking-results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1663508957162,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "BwX2W_UuAPso",
    "outputId": "eac82eeb-3ac8-45aa-8247-66eb88c55b2e"
   },
   "outputs": [],
   "source": [
    "config_file = configparser.ConfigParser()  # CREATE OBJECT\n",
    "config_file.read(\"Train Configurations.ini\")  # READ THE CONFIG FILE\n",
    "\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"DATA\"]={\n",
    "        \"raw_data_path\": 'Data/Train',  # Directory containing Raw Data\n",
    "        \"augment_data_path\": 'Data/Train',  # Directory for the Images to be Augmented\n",
    "        \"augmentation_num\": 10  # Number of Augmentations per image\n",
    "        }\n",
    "\n",
    "# SAVE THE SETTINGS TO THE FILE\n",
    "with open(\"Train Configurations.ini\",\"w\") as file_object:\n",
    "    config_file.write(file_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1663508957162,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "zJ6WGnmJjewG",
    "outputId": "55779234-4198-4444-cd5f-f58b3982fcdf"
   },
   "outputs": [],
   "source": [
    "'Configurations'\n",
    "config_file = configparser.ConfigParser()  # CREATE OBJECT\n",
    "config_file.read(\"Train Configurations.ini\")  # READ THE CONFIG FILE\n",
    "augment_data_path = config_file[\"DATA\"][\"augment_data_path\"]  # Augmentation is normally done on the Train Data but Test and Validation sets can be processed as well (e.g., resize) by changing the directory\n",
    "augmentation_num = int(config_file[\"DATA\"][\"augmentation_num\"])  # Number of Augmentations per Image\n",
    "\n",
    "'List of Augmentations to be applied per image, vary it according to the requirements'\n",
    "augmentation_list = [A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=1.0),\n",
    "                     A.Rotate(limit=90, interpolation=1, border_mode=4, p=1.0),\n",
    "                     A.Flip(p=0.5),\n",
    "                     A.Transpose(p=0.5),\n",
    "                     # A.RandomSizedCrop(min_max_height=[384,384], height=512, width=512, p=0.5),\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsLpjnEFtKpk"
   },
   "source": [
    "Augment Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJsxaaFymx0C"
   },
   "outputs": [],
   "source": [
    "def augment_segmentation_data(data_path, augmentation_list, augmentation_num):\n",
    "    num_folds = len(os.listdir(data_path))\n",
    "    # Declare an Augmentation Pipeline\n",
    "    for i in range(1, num_folds + 1):\n",
    "        print(f'\\nCurrently Processing Fold {i}')\n",
    "        # List containing all images of a certain class in a certain fold\n",
    "        Image_List = sorted(os.listdir(f'{data_path}/Fold_{i}/Images/'))\n",
    "        Mask_List = sorted(os.listdir(f'{data_path}/Fold_{i}/Masks/'))\n",
    "        for ii in range(0, (len(Mask_List))):\n",
    "            current_image = Image_List[ii]\n",
    "            current_image = current_image[0:-4]\n",
    "            current_mask = Mask_List[ii]\n",
    "            current_mask = current_mask[0:-4]\n",
    "            # Read an image with OpenCV and convert it to the RGB colorspace\n",
    "            org_image = cv2.imread(f'{data_path}/Fold_{i}/Images/{Image_List[ii]}')  # Read Original Image\n",
    "            corresponding_mask = cv2.imread(f'{data_path}/Fold_{i}/Masks/{Mask_List[ii]}')  # Read Original Image\n",
    "            img_nparray = np.asarray(org_image)\n",
    "            if img_nparray.shape[2] > 3:\n",
    "                org_image = org_image[:,:,:3]  # Remove Alpha (4th) Channel from the Image\n",
    "            org_image = cv2.cvtColor(org_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB Colorspace\n",
    "            msk_nparray = np.asarray(corresponding_mask)\n",
    "            if msk_nparray.shape[2] > 1:\n",
    "                corresponding_mask = corresponding_mask[:,:,0]  # Convert masks to Grayscale images from RGB\n",
    "            for iv in range(1, augmentation_num + 1):\n",
    "                transform = A.Compose(augmentation_list)\n",
    "                transformed_GT = transform(image=org_image, mask=corresponding_mask)  # Augment Image and Corresponding Mask\n",
    "                transformed_image = transformed_GT['image']\n",
    "                transformed_mask = transformed_GT['mask']\n",
    "                cv2.imwrite(f'{data_path}/Fold_{i}/Images/{current_image}_Augmented_{iv}.png', transformed_image)\n",
    "                cv2.imwrite(f'{data_path}/Fold_{i}/Masks/{current_mask}_Augmented_{iv}.png', transformed_mask)\n",
    "            print(f'Image Number: {ii+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 907941,
     "status": "ok",
     "timestamp": 1663509865098,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "z9O402pXjiUY",
    "outputId": "b2614bbd-cc0b-482a-fd12-31f503f5fae5"
   },
   "outputs": [],
   "source": [
    "# from Helper_Functions import augment_segmentation_data\n",
    "augment_segmentation_data(augment_data_path, augmentation_list, augmentation_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtI0XX-ZB1ok"
   },
   "source": [
    "### Check and Visualize Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1663300320608,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "xx76fg_DB1ok",
    "outputId": "37fe0109-698b-466b-9a44-bc08c9993338"
   },
   "outputs": [],
   "source": [
    "# Open the image form working directory\n",
    "image_path = '/content/Data/Train/Fold_1/Images/1.2.392.200036.9116.6.23.10552866.1142.20210815074059931.1.433.png'\n",
    "mask_path = '/content/Data/Train/Fold_1/Masks/1.2.392.200036.9116.6.23.10552866.1142.20210815074059931.1.433.png'\n",
    "# load and show an image with Pillow\n",
    "image = PIL.Image.open(image_path)\n",
    "print(image.mode)\n",
    "# convert image to numpy array\n",
    "data = np.asarray(image)\n",
    "print(type(data))  # Type of the Image (e.g., RGB, BGR or RGBA)\n",
    "# summarize shape\n",
    "print(data.shape)  # Shape of the Images\n",
    "print(np.max(data[:,:]))  # Compute Pixel information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1663300324895,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "2Cr_txKlkb5C",
    "outputId": "8229c962-9dfc-4447-be89-ef98f34a201b"
   },
   "outputs": [],
   "source": [
    "# load and show an image with Pillow\n",
    "image = PIL.Image.open(mask_path)\n",
    "print(image.mode)\n",
    "# convert image to numpy array\n",
    "data = np.asarray(image)\n",
    "print(type(data))  # Type of the Image (e.g., RGB, BGR or RGBA)\n",
    "# summarize shape\n",
    "print(data.shape)  # Shape of the Images\n",
    "print(np.max(data[:,:]))  # Compute Pixel information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViyXcDpDB1ok"
   },
   "source": [
    "Plot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "executionInfo": {
     "elapsed": 790,
     "status": "ok",
     "timestamp": 1663300327380,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "2OND0lcaB1ok",
    "outputId": "0c459bfc-b381-45f1-c5c7-92886b41cf6a"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow  # This is used to display images in COLAB using cv2\n",
    "image = cv2.imread(image_path)\n",
    "cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "executionInfo": {
     "elapsed": 881,
     "status": "ok",
     "timestamp": 1663300345774,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "U87OtvU_h7kJ",
    "outputId": "57fafdc1-1d29-44d5-b876-0c9f889469d3"
   },
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "figure(figsize=(15, 8), dpi=90)\n",
    "\n",
    "image1 = plt.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "image2 = plt.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "plt.subplot(1, 2, 1), plt.imshow(image1, 'gray')\n",
    "plt.grid(False)\n",
    "plt.title(f\"Kidney US Image\")\n",
    "plt.subplot(1, 2, 2), plt.imshow(image2, 'gray')\n",
    "plt.title(f\"Corresponding GT Mask\")\n",
    "plt.axis('off')\n",
    "plt.show() # To show figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnWmDobgB1ol"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko6pyUDvB1om"
   },
   "source": [
    "### Train Configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "RaOJANaauzsb"
   },
   "source": [
    "**Available 2D-Segmentation Models:**  \n",
    "\n",
    "**--- UNet Variants ---**\n",
    "* **UNet**\n",
    "* **Ensemble UNet (UNetE)**\n",
    "* **UNet+**\n",
    "* **UNet++**\n",
    "* **UNet3+**\n",
    "* **UNet4+**\n",
    "* **AHNet**\n",
    "* **KSSNet**\n",
    "* **MultiResUNet**\n",
    "* **MultiResUNet3+**\n",
    "* **Self-UNet**\n",
    "* **Self-UNet++**\n",
    "* **Self-UNet3+**  \n",
    "\n",
    "**--- FPN Variants ---**\n",
    "* **FPN**\n",
    "* **MultiResFPN**\n",
    "* **Self-FPN**\n",
    "\n",
    "**Supported PreTrained Encoders:**\n",
    "* **ResNet:** ResNet50, ResNet101, ResNet152, ResNet50V2, ResNet101V2, ResNet152V2 (ImageNet)\n",
    "* **VGG:** VGG16, VGG19 (ImageNet)\n",
    "* **DenseNet:** DenseNet121, DenseNet169, DenseNet201 (ImageNet)\n",
    "* **MobileNet:** MobileNet, MobileNetV2, MobileNetV3Small, MobileNetV3Large (ImageNet)\n",
    "* **Inception:** InceptionV3, InceptionResNetV2 (ImageNet)\n",
    "* **EfficientNetV1:** EfficientNetB[0-7] (ImageNet)\n",
    "* **EfficientNetV2:** EfficientNetV2B[0-3], EfficientNetV2S, EfficientNetV2M, EfficientNetV2L (ImageNet)\n",
    "* **CheXNet** (Custom)\n",
    "\n",
    "**Supported Loss Functions:** Check \"tf_losses.py\" file  \n",
    "**Supported Optimizers:** Check \"tf_optimizers.py\" file  \n",
    "**Supported Metrics:** Check \"tf_metrics.py\" file  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510,
     "status": "ok",
     "timestamp": 1663574551604,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "lW6u8UNfB1om",
    "outputId": "e50e0f89-8853-444c-9b0c-88326ed996f0"
   },
   "outputs": [],
   "source": [
    "# CREATE OBJECT\n",
    "config_file = configparser.ConfigParser()\n",
    "\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"TRAIN\"] = {\n",
    "        ## Data Configurations\n",
    "        \"train_dir\": \"Data/Train\",  # Train Directory\n",
    "        \"val_dir\": \"Data/Val\",  # Validation Directory\n",
    "        \"independent_val_set\": True,  # True: Independent Validation Set | False: Validation Set randomly splitted from the Training Set\n",
    "        \"validation_portion\": 0.0,  # 0 to 1 [Default: 0; when validation set is independent, otherwise created randomly while training based on \"validation_portion\"]\n",
    "        \"imlength\": 512,  # Length or Height of the Image | Image Size: [imwidth, imlength]\n",
    "        \"imwidth\": 512,  # Width of the Image\n",
    "        \"image_color_mode\": \"rgb\",  # Color Mode of the images [rgb, rgba (rgb with transparent alpha channel), grayscale (black and white single channel image)]\n",
    "        \"mask_color_mode\": \"grayscale\",  # Color Mode of the masks [rgb or grayscale (black and white single channel image)]\n",
    "        \"num_channels\": 3,  # Number of Input Channels in the Model [rgb:3, rgba:4, grayscale:1]\n",
    "        \"normalizing_factor_img\": 255,  # 255.0 for images with pixel values varying between 0 to 255. If it is between 0 to 1, change it to 1\n",
    "        \"normalizing_factor_msk\": 1,  # 255 for masks with pixel values varying between 0 to 255. If it is between 0 to 1, change it to 1\n",
    "        ## Model Configurations\n",
    "        \"model_genre\": \"UNet\",  # model_genre: Generation or Genre of the Model: UNet, FPN, LinkNet, etc.\n",
    "        # Encoder\n",
    "        \"encoder_mode\": \"from_scratch\",  # Transfer Learning: \"pretrained_encoder\" | Train from scratch: \"from_scratch\"\n",
    "        \"encoder_name\": \"ResNet50\",  # Select an Encoder from a pool of ImageNet trained Models available from TensorFlow, default: ResNet50\n",
    "        \"encoder_trainable\": False,  # Fine Tuning ON/OFF [True/False] | Start with OFF, Fine Tune later in the 2nd stage, which is optional\n",
    "        # Decoder\n",
    "        \"decoder_name\": \"KSSNet\",  # Select a Model from the list to train from scratch, UNet is kept as default\n",
    "        \"model_width\": 16,  # Number of Filters or Kernels of the Input Layer, subsequent layers start from here\n",
    "        \"model_depth\": 5,  # Number of Layers in the Model [For the \"pretrained_encoder\" mode: Maximum 5, Minimum 1]\n",
    "        \"output_nums\": 1,  # Number of Outputs for the model\n",
    "        \"A_E\": 0,  # Turn on AutoEncoder Mode for Feature Extraction [Default: 0]\n",
    "        \"A_G\": 0,  # Turn on for Guided Attention [Default: 0]\n",
    "        \"LSTM\": 0,  # Turn on for LSTM [Default: 0]\n",
    "        \"dense_loop\": 2,  # Number of Densely Connected Residual Blocks in the BottleNeck Layer [Default: 2]\n",
    "        \"feature_number\": 1024,  # Number of Features to be Extracted [Only required for the AutoEncoder (A_E) Mode]\n",
    "        \"is_transconv\": True,  # True: Transposed Convolution | False: UpSampling in the Decoder layer\n",
    "        \"alpha\": 1,  # Alpha parameter, required for MultiResUNet models [Default: 1]\n",
    "        \"q_onn\": 3,  # 'q' for Self-ONN' [Default: 3, set 1 to get CNN]\n",
    "        \"final_activation\": \"sigmoid\",  # Activation Function for the Final Layer: \"Linear\", \"Sigmoid\", \"Softmax\", etc. depending on the problem type\n",
    "        \"class_number\": 2,  # Number of Output Classes [e.g., here for Kidney Tumor segmentation, Class 0: Background | Class 1: Kidney | Class 2: Tumor]\n",
    "        \"target_class\": 1,  \n",
    "        ## Training Configurations\n",
    "        \"batch_size\": 4,  # Batch Size of the Images being loaded for training\n",
    "        \"learning_rate\": 0.0005,  # During Fine-Tuning the network, the Learning Rate should be very low (e.g., 1e-5), otherwise more (e.g., 1e-4, 1e-3)\n",
    "        \"start_fold\": 3,  # Fold to Start Training, can be varied from 1 to the last fold\n",
    "        \"end_fold\": 5,  # Fold to End Training, can be any value from the start_fold [Number of Folds + 1]\n",
    "        \"monitor_param\": \"val_out_mean_squared_error\",  # Monitoring parameter during training\n",
    "        \"patience_amount\": 20,  # Number of epochs to wait before training to stop\n",
    "        \"patience_amount_RLROnP\": 10,  # Number of epochs to wait before training to stop\n",
    "        \"patience_mode\": \"min\",  # patience mode: 'min', 'max' or 'auto'\n",
    "        \"RLROnP_factor\": 0.1,  # Number of epochs to wait before training to stop\n",
    "        \"num_epochs\": 200,  # Number of epochs for training\n",
    "        \"loss_function\": \"BinaryCrossentropy\", # Loss Functions\n",
    "        \"optimizer_function\": \"Adam\",  # Optimization Algorithm\n",
    "        \"metric_list\": \"MeanSquaredLogarithmicError\",  # Metric(s) being monitored\n",
    "        \"save_history\": True,  # Metric(s) being monitored\n",
    "        \"load_weights\": False,  # Metric(s) being monitored\n",
    "        \"save_dir\": \"Results\",  # Metric(s) being monitored\n",
    "        \"task_name\": \"None\",  # Default: None\n",
    "        \"seed\": 1,  # SEED required for randomly split Validation set from the Training set, not used when \"validation_portion\"= 0.0\n",
    "        # Patchify\n",
    "        \"patchify\": False,  # Metric(s) being monitored\n",
    "        \"patch_width\": 64,  # Length or Height of the Image | Image Size: [imwidth, imlength]\n",
    "        \"patch_height\": 64,  # Width of the Image\n",
    "        \"overlap_ratio\": 0,\n",
    "        # Deep Supervision\n",
    "        \"D_S\": 0,  # Turn on Deep Supervision [Default: 0]\n",
    "        \"ds_type\": \"UNetPP\"  # \"UNet\" or \"UNetPP\"; only required when Deep Supervision (D_S) is on\n",
    "        }\n",
    "# SAVE CONFIG FILE\n",
    "with open(r\"Train_Configs.ini\", 'w') as configfileObj:\n",
    "    config_file.write(configfileObj)\n",
    "    configfileObj.flush()\n",
    "    configfileObj.close()\n",
    "print(\"Config file 'Train_Configs.ini' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YSDnLOIWB1om"
   },
   "source": [
    "### Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W-hn_PogB1ov"
   },
   "source": [
    "Major Flexible Training Loop Accross Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11394443,
     "status": "ok",
     "timestamp": 1663573505248,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "0QvnqKJJilrJ",
    "outputId": "612131a7-8bfa-4bbb-bfc7-42a0a2466fee"
   },
   "outputs": [],
   "source": [
    "%run -m train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnvgnHUnPPS9"
   },
   "source": [
    "### Fine-Tune Model (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Au1Kg6Y6PIOa"
   },
   "source": [
    "Update Training Configurations for the Fine-Tuning Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1663573505248,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "KflERmsBPIH5",
    "outputId": "d2d19525-b7fa-4165-aea1-82fc318a7f7a"
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('Train_Configs.ini')\n",
    "config.set('TRAIN', 'encoder_trainable', 'True')\n",
    "config.set('TRAIN', 'learning_rate', '0.00005')\n",
    "config.set('TRAIN', 'patience_amount', '10')\n",
    "config.set('TRAIN', 'num_epochs', '50')\n",
    "config.set('TRAIN', 'save_history', 'False')\n",
    "\n",
    "with open('Train_Configs.ini', 'w') as configfile:\n",
    "    config.write(configfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yF8l-VSPSB4"
   },
   "source": [
    "Fine-Tune Model through Re-training\n",
    "* Make sure a previously trained model is loaded\n",
    "* Fine-tuning a previously trained model after unfreezing the previously frozen layers of the trained encoder should be done with a very low learning rate (e.g., 10X-20X lower than the original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1HqxypZPSHo"
   },
   "outputs": [],
   "source": [
    "%run -m train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_tDBaqIWB1ox"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJhEvsxKB1oy"
   },
   "source": [
    "Set Test Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1663574155633,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "XdDrlOm9AAoJ",
    "outputId": "b56add67-a2fe-4521-ca08-db2d8f8cff6e"
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# CREATE OBJECT \n",
    "config_file = configparser.ConfigParser()\n",
    "\n",
    "# ADD NEW SECTION AND SETTINGS\n",
    "config_file[\"TEST\"] = {\n",
    "        ## Data Configurations\n",
    "        \"test_dir\": \"Data/Test\",\n",
    "        \"imheight\": 512,  # Size of the Images being Trained, they will be resized in this shape: [imsize, imsize]\n",
    "        \"imwidth\": 512,  # Size of the Images being Trained, they will be resized in this shape: [imsize, imsize]\n",
    "        \"image_color_mode\": \"rgb\",  # Color Mode of the images [rgb, rgba (rgb with transparent alpha channel), grayscale (black and white single channel image)]\n",
    "        \"mask_color_mode\": \"grayscale\",  # Color Mode of the kmasks [rgb or grayscale (black and white single channel image)]\n",
    "        \"num_channels\": 3,  # Number of Input Channels in the Model [rgb:3, rgba:4, grayscale:1]\n",
    "        \"class_number\": 2,  # Number of Output Classes [e.g., here for Kidney Tumor segmentation, Class 0: Background | Class 1: Kidney | Class 2: Tumor]\n",
    "        \"labels\": \"\",\n",
    "        ## Model Configurations\n",
    "        \"encoder_mode\": \"from_scratch\",  # Transfer Learning: \"pretrained_encoder\" | Train from scratch: \"from_scratch\"\n",
    "        \"encoder_name\": 'None', # Select an Encoder from a pool of ImageNet trained Models available from TensorFlow, default: ResNet50\n",
    "        \"decoder_name\": \"UNet3P\",  # Select a Model from the list to train from scratch, ResNet50 is kept as default\n",
    "        ## Test Configurations\n",
    "        \"batch_size\": 4,  # Batch Size of the Images bein loaded for training\n",
    "        \"normalizing_factor_img\": 255.0,  # 255.0 for images with pixel values varying between 0 to 255. If it is between 0 to 1, change it to 1\n",
    "        \"normalizing_factor_msk\": 1,  # 255.0 for images with pixel values varying between 0 to 255. If it is between 0 to 1, change it to 1\n",
    "        \"start_fold\": 1,  # Fold to Start Training, can be varied from 1 to the last fold\n",
    "        \"end_fold\": 5,  # Fold to End Training, can be any value from the start_fold [Number of Folds + 1]\n",
    "        \"num_iter\": 1,  # Number of Folds completed training\n",
    "        \"threshold\": 0.5,  # Set Threshold for Decision Making\n",
    "        \"seed\": 1,  # SEED required for randomly split Validation set from the Training set, not used when \"validation_portion\"= 0.0\n",
    "        \"save_dir\": 'Results',  # Directory for saving inference outcomes\n",
    "        # Patchify\n",
    "        \"patchify\": False,  # Metric(s) being monitored\n",
    "        \"patch_width\": 64,  # Length or Height of the Image | Image Size: [imwidth, imlength]\n",
    "        \"patch_height\": 64,  # Width of the Image\n",
    "        \"overlap_ratio\": 0,  # Should always be 'Zero' during evaluation\n",
    "        # Deep Supervision\n",
    "        \"D_S\": 0  # Turn on Deep Supervision [Default: 0]\n",
    "        }\n",
    "\n",
    "# SAVE CONFIG FILE\n",
    "with open(r\"Test_Configs.ini\", 'w') as configfileObj:\n",
    "    config_file.write(configfileObj)\n",
    "    configfileObj.flush()\n",
    "    configfileObj.close()\n",
    "\n",
    "print(\"Config file 'Test_Configs.ini' created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYjMZ2V8cjhw"
   },
   "source": [
    "Test and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12350,
     "status": "ok",
     "timestamp": 1663574358745,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "Lxgzvj2Ct6VF",
    "outputId": "e469a41c-ccdc-4616-d417-27d5ba4091ad"
   },
   "outputs": [],
   "source": [
    "%run -m test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Masks for Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to visible masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations\n",
    "rootdir = 'Results/Stone/UNet4P_from_scratch_DS/Fold_1/Predictions/'\n",
    "savedir = 'Outcomes/Stone/Fold_1/UNet4P_from_Scratch_DS/'\n",
    "# Iterate over files in directory\n",
    "for name in os.listdir(rootdir):\n",
    "    print(name)\n",
    "    fullfile_dir = rootdir + name\n",
    "    image_ = cv2.imread(fullfile_dir)\n",
    "    img_arr = np.array(image_)\n",
    "    img_arr[img_arr == 1] = 255\n",
    "    # print(np.unique(img_arr))\n",
    "    cv2.imwrite(savedir + name, img_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Multiclass Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations\n",
    "rootdir_kidney_masks = 'Outcomes/Kidney/Fold_1/UNet4P_from_Scratch_DS/'\n",
    "rootdir_stone_masks = 'Outcomes/Stone/Fold_1/UNet4P_from_Scratch_DS/'\n",
    "savedir = 'Outcomes/Combined/Fold_1/UNet4P_from_Scratch_DS/'\n",
    "alpha = 1\n",
    "beta = 1\n",
    "gamma = 0\n",
    "# Iterate over files in directory\n",
    "for name in os.listdir(rootdir_kidney_masks):\n",
    "    print(name)\n",
    "    fullfile_dir_kidney_mask = rootdir_kidney_masks + name\n",
    "    fullfile_dir_stone_mask = rootdir_stone_masks + name\n",
    "    image_kidney = cv2.imread(fullfile_dir_kidney_mask)\n",
    "    # img_kidney_arr = np.array(image_kidney)\n",
    "    image_stone = cv2.imread(fullfile_dir_stone_mask)\n",
    "    # image_stone_arr = np.array(image_stone)\n",
    "    image_combine = cv2.addWeighted(image_kidney, alpha, image_stone, beta, gamma)\n",
    "    image_combine_arr = np.asarray(image_combine, dtype=np.int16)\n",
    "    # print(np.unique(image_combine_arr))\n",
    "    cv2.imwrite(savedir + name, image_combine_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare GT and Pred Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import binary_erosion, distance_transform_edt\n",
    "\n",
    "def assd(y_true, y_pred, spacing=None):\n",
    "    \"\"\"\n",
    "    Average Symmetric Surface Distance (ASSD) between two binary masks (2D or 3D).\n",
    "    Returns float (same units as `spacing`, or pixels/voxels if spacing=None).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(bool)\n",
    "    y_pred = np.asarray(y_pred).astype(bool)\n",
    "\n",
    "    # 1. Extract surfaces\n",
    "    footprint = np.ones((3,) * y_true.ndim, dtype=bool)\n",
    "    s_true = y_true ^ binary_erosion(y_true, structure=footprint, border_value=0)\n",
    "    s_pred = y_pred ^ binary_erosion(y_pred, structure=footprint, border_value=0)\n",
    "\n",
    "    # 2. Handle empty cases\n",
    "    if not s_true.any() and not s_pred.any():\n",
    "        return 0.0\n",
    "    if not s_true.any() or not s_pred.any():\n",
    "        return float(\"inf\")\n",
    "\n",
    "    # 3. Calculate distance transforms\n",
    "    dt_true = distance_transform_edt(~s_true, sampling=spacing)\n",
    "    dt_pred = distance_transform_edt(~s_pred, sampling=spacing)\n",
    "\n",
    "    # 4. Standard ASSD Calculation (Global Mean)\n",
    "    # Total sum of distances / Total number of points\n",
    "    sum_dist = dt_true[s_pred].sum() + dt_pred[s_true].sum()\n",
    "    total_points = s_pred.sum() + s_true.sum()\n",
    "    \n",
    "    return sum_dist / total_points\n",
    "\n",
    "def hd95(y_true, y_pred, spacing=None):\n",
    "    \"\"\"\n",
    "    95th percentile Hausdorff Distance (HD95) between two binary masks (2D or 3D).\n",
    "    Returns float (same units as `spacing`, or pixels/voxels if spacing=None).\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true).astype(bool)\n",
    "    y_pred = np.asarray(y_pred).astype(bool)\n",
    "    \n",
    "    # Surface = mask XOR eroded(mask)\n",
    "    footprint = np.ones((3,) * y_true.ndim, dtype=bool)\n",
    "    s_true = y_true ^ binary_erosion(y_true, structure=footprint, border_value=0)\n",
    "    s_pred = y_pred ^ binary_erosion(y_pred, structure=footprint, border_value=0)\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if not s_true.any() and not s_pred.any():\n",
    "        return 0.0\n",
    "    if not s_true.any() or not s_pred.any():\n",
    "        return float(\"inf\")  # undefined; one surface missing\n",
    "\n",
    "    dt_true = distance_transform_edt(~s_true, sampling=spacing)\n",
    "    dt_pred = distance_transform_edt(~s_pred, sampling=spacing)\n",
    "\n",
    "    d_pred_to_true = dt_true[s_pred]  # distances of pred surface points to true surface\n",
    "    d_true_to_pred = dt_pred[s_true]  # distances of true surface points to pred surface\n",
    "\n",
    "    # Standard Symmetric HD95: Max of the two directed 95th percentiles\n",
    "    hd95_1 = np.percentile(d_pred_to_true, 95)\n",
    "    hd95_2 = np.percentile(d_true_to_pred, 95)\n",
    "    \n",
    "    return max(hd95_1, hd95_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations\n",
    "gt_stone_masks_dir = 'Results/Masks/'\n",
    "pred_stone_masks_dir = 'Results/Predictions/'\n",
    "sub_list = []\n",
    "dsc_all = []\n",
    "iou_all = []\n",
    "assd_all = []\n",
    "hd95_all = []\n",
    "dsc_all_arr = []\n",
    "iou_all_arr = []\n",
    "assd_all_arr = []\n",
    "hd95_all_arr = []\n",
    "dsc_current_sub = []\n",
    "iou_current_sub = []\n",
    "assd_current_sub = []\n",
    "hd95_current_sub = []\n",
    "col_names = ['Subject No.',\n",
    "             'DSC',\n",
    "             'IoU',\n",
    "             'ASSD',\n",
    "             'HD95']\n",
    "df_stone_segmentation_performance = pd.DataFrame(columns=col_names)\n",
    "prev_sub_num = '0'\n",
    "for name in os.listdir(gt_stone_masks_dir):\n",
    "    current_sub_num = name.split(\"_\")[0]\n",
    "    current_gt_mask_dir = gt_stone_masks_dir + name\n",
    "    current_pred_mask_dir = pred_stone_masks_dir + name\n",
    "    gt_mask_current_arr = np.asarray(cv2.imread(current_gt_mask_dir), dtype=np.int16).ravel()\n",
    "    pred_mask_current_arr = np.asarray(cv2.imread(current_pred_mask_dir), dtype=np.int16).ravel()\n",
    "    intersection = tf.reduce_sum(gt_mask_current_arr * pred_mask_current_arr)\n",
    "    union = tf.reduce_sum(gt_mask_current_arr) + tf.reduce_sum(pred_mask_current_arr)\n",
    "    dsc = np.asarray(((2 * intersection) / union), dtype=np.float64)\n",
    "    iou = jaccard_score(gt_mask_current_arr, pred_mask_current_arr)\n",
    "    # print(np.unique(gt_mask_current_arr), np.unique(pred_mask_current_arr))\n",
    "    ASSD_ = assd(gt_mask_current_arr, pred_mask_current_arr)\n",
    "    HD95_ = hd95(gt_mask_current_arr, pred_mask_current_arr)\n",
    "    if current_sub_num == prev_sub_num:\n",
    "        dsc_current_sub.append(dsc)\n",
    "        iou_current_sub.append(iou)\n",
    "        assd_current_sub.append(ASSD_)\n",
    "        hd95_current_sub.append(HD95_)\n",
    "        dsc_all_arr.append(dsc)\n",
    "        iou_all_arr.append(iou)\n",
    "        assd_all_arr.append(ASSD_)\n",
    "        hd95_all_arr.append(HD95_)\n",
    "    else:\n",
    "        dsc_prev_sub = np.round(np.mean(np.array(dsc_current_sub))*100, 2)\n",
    "        iou_prev_sub = np.round(np.mean(np.array(iou_current_sub))*100, 2)\n",
    "        assd_prev_sub = np.round(np.mean(np.array(assd_current_sub)), 4)\n",
    "        hd95_prev_sub = np.round(np.mean(np.array(hd95_current_sub)), 4)\n",
    "        sub_list.append(prev_sub_num)\n",
    "        dsc_all.append(dsc_prev_sub)\n",
    "        iou_all.append(iou_prev_sub)\n",
    "        assd_all.append(assd_prev_sub)\n",
    "        hd95_all.append(hd95_prev_sub)\n",
    "        print(prev_sub_num, dsc_prev_sub, iou_prev_sub, assd_prev_sub, hd95_prev_sub)\n",
    "        # Update per subject arrays\n",
    "        dsc_current_sub = []\n",
    "        iou_current_sub = []\n",
    "        assd_current_sub = []\n",
    "        hd95_current_sub = []\n",
    "        dsc_current_sub.append(dsc)\n",
    "        iou_current_sub.append(iou)\n",
    "        assd_current_sub.append(ASSD_)\n",
    "        hd95_current_sub.append(HD95_)\n",
    "        dsc_all_arr.append(dsc)\n",
    "        iou_all_arr.append(iou)\n",
    "        assd_all_arr.append(ASSD_)\n",
    "        hd95_all_arr.append(HD95_)\n",
    "    prev_sub_num = current_sub_num                                                                                                                                                                           \n",
    "    #\n",
    "df_stone_segmentation_performance['Subject No.'] = sub_list\n",
    "df_stone_segmentation_performance['DSC'] = dsc_all\n",
    "df_stone_segmentation_performance['IoU'] = iou_all\n",
    "df_stone_segmentation_performance['ASSD'] = assd_all\n",
    "df_stone_segmentation_performance['HD95'] = hd95_all\n",
    "filepath = Path('Kidney_Stone_Segmentation_Performance.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df_stone_segmentation_performance.to_csv(filepath, index=False)\n",
    "dsc_all_mean = np.round(np.mean(np.array(dsc_all_arr))*100, 2)\n",
    "iou_all_mean = np.round(np.mean(np.array(iou_all_arr))*100, 2)\n",
    "assd_all_mean = np.round(np.mean(np.array(assd_all_arr)), 4)\n",
    "hd95_all_mean = np.round(np.mean(np.array(hd95_all_arr)), 4)\n",
    "print(dsc_all_mean, iou_all_mean, assd_all_mean, hd95_all_mean)\n",
    "# Print/Show the DataFrame\n",
    "print(df_stone_segmentation_performance.shape)\n",
    "df_stone_segmentation_performance.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "confusion_matrix_raw = np.array([[16522,1382],[325,46759]])\n",
    "confusion_matrix_norm = np.array([[0.92,0.08],[0.01,0.99]])\n",
    "labels = ['Kidney', 'No Kidney']\n",
    "print(confusion_matrix_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "shape = confusion_matrix_raw.shape\n",
    "data = np.asarray(confusion_matrix_raw, dtype=int)\n",
    "text = np.asarray(confusion_matrix_norm, dtype=float)\n",
    "annots = (np.asarray([\"{0:.2f} ({1:.0f})\".format(text, data) for text, data in zip(text.flatten(), data.flatten())])).reshape(shape[0],shape[1])\n",
    "fig = plt.figure(figsize=(len(labels)*5, len(labels)*4))\n",
    "sns.heatmap(confusion_matrix_norm, cmap='Blues', annot=annots, fmt='', annot_kws={'fontsize': 36}, xticklabels=labels, yticklabels=labels, vmax=1)\n",
    "sns.set(font_scale=1.8)\n",
    "plt.title('Confusion Matrix', fontsize=36)\n",
    "plt.xlabel(\"Predicted\", fontsize=30)\n",
    "plt.ylabel(\"True\", fontsize=30)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lC-4-y6IB1oz"
   },
   "source": [
    "#### Get Pixel Array from Image Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gss6OD6oB1oz"
   },
   "source": [
    "##### Get Pixels only inside the Field of Vision (FoV) of the GT Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9620,
     "status": "ok",
     "timestamp": 1640281083041,
     "user": {
      "displayName": "Sakib Mahmud",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg8lG2uTygQr7y6fmQUo67XXUtrCVGaEakj_P33Ft8=s64",
      "userId": "03961007737707022852"
     },
     "user_tz": -180
    },
    "id": "1YUMdhILB1oz",
    "outputId": "bd1d2e86-ca8e-4736-aee1-cca01d97cb16"
   },
   "outputs": [],
   "source": [
    "# Predictions only inside the FOV by removing the black background\n",
    "y_scores, y_true = pred_only_FOV(pred_imgs, gtruth_masks, test_border_masks)  # Returns data only inside the FOV\n",
    "print(\"Calculating results only inside the FOV:\")\n",
    "print(\"y-scores pixels: \" +str(y_scores.shape[0]) + \" | Including Background: \" +str(pred_imgs.shape[0]*pred_imgs.shape[1]*pred_imgs.shape[2]))\n",
    "print(\"y-true pixels: \" +str(y_true.shape[0]) + \" | Including Background: \" +str(gtruth_masks.shape[0]*gtruth_masks.shape[1]*gtruth_masks.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGtzpI_3B1o0"
   },
   "source": [
    "##### Get all Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1654870771458,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "rmExqe3yB1o0",
    "outputId": "162770d5-cf2a-49de-f2d6-b09dd627e8f6"
   },
   "outputs": [],
   "source": [
    "# Get all Pixels\n",
    "y_true = np.asarray(Y_Test.ravel())\n",
    "y_scores = np.asarray(Predictions.ravel())\n",
    "print(\"Calculating results for All Pixels:\")\n",
    "print(\"y-scores pixels: \" + str(y_scores.shape[0]))\n",
    "print(\"y-true pixels: \" + str(y_true.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mcqaz5aY-y8f"
   },
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "y_true = np.where(y_true > threshold, 1, 0)\n",
    "y_pred = np.where(y_scores > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qa8CQdxB1o0"
   },
   "source": [
    "#### Visualize Pixel Density or Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mteRfLZoB1o0"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'Ground Truth': y_true, 'Predictions': y_scores})\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.set_style('whitegrid')\n",
    "sns.kdeplot(data=df)\n",
    "plt.title('KDE Plot for Ground Truth and Predictions', fontsize=20)\n",
    "plt.xlabel('Magnitude', fontsize=15)\n",
    "plt.ylabel('Density', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JaNvB4OB1o1"
   },
   "source": [
    "Violin Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5DpOvC39B1o1"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "df = pd.DataFrame(data = {'Ground Truth': y_true, 'Predictions': y_scores})\n",
    "ax = sns.violinplot(data=df)\n",
    "plt.title('Violin Plot for Ground Truth and Predictions', fontsize=20)\n",
    "plt.ylabel('Magnitude', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9y1zqdGB1o3"
   },
   "source": [
    "#### Visualize Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVy-25I5B1o3"
   },
   "source": [
    "Plot GT Image, Mask and Predicted Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rB8McPdB1o3"
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig,ax = plt.subplots(20,4,figsize=[20,50])\n",
    "\n",
    "for idx in range(20):\n",
    "    offset = 20\n",
    "    ax[idx,0].imshow(np.float32(np.squeeze((orig_imgs[idx+offset]))), cmap='gray')\n",
    "    ax[idx,1].imshow(np.squeeze(gtruth_masks[idx+offset]), cmap='gray')\n",
    "    ax[idx,2].imshow(np.squeeze(pred_imgs[idx+offset]), cmap='gray')\n",
    "    img = np.float32(np.squeeze(pred_imgs[idx+offset]))\n",
    "    ret, thresh = cv2.threshold(img,0.5,1,cv2.THRESH_BINARY)\n",
    "    ax[idx,3].imshow(thresh, cmap='gray')\n",
    "    ax[idx,0].set_title(f\"Input Image {idx+offset}\")\n",
    "    ax[idx,1].set_title(f\"Ground Truth Mask {idx+offset}\")\n",
    "    ax[idx,2].set_title(f\"Predicted Mask {idx+offset}\")\n",
    "    ax[idx,3].set_title(f\"Thresholded Predicted Mask {idx+offset}\")\n",
    "    ax[idx,0].axis(\"off\")\n",
    "    ax[idx,1].axis(\"off\")\n",
    "    ax[idx,2].axis(\"off\")\n",
    "    ax[idx,3].axis(\"off\")\n",
    "\n",
    "plt.savefig(path_experiment+f'Sample_Results_for_Test_Images_{offset}_to_{idx+offset}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iB4fGHuTB1o3"
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig,ax = plt.subplots(1,4,figsize=[30,10])\n",
    "\n",
    "idx = 76\n",
    "ax[0].imshow(np.float32(np.squeeze((orig_imgs[idx]))), cmap='gray')\n",
    "ax[1].imshow(np.squeeze(gtruth_masks[idx]), cmap='gray')\n",
    "ax[2].imshow(np.squeeze(pred_imgs[idx]), cmap='gray')\n",
    "img = np.float32(np.squeeze(pred_imgs[idx]))\n",
    "ret, thresh = cv2.threshold(img,0.4,1,cv2.THRESH_BINARY)\n",
    "ax[3].imshow(thresh, cmap='gray')\n",
    "ax[0].set_title(f\"Input Image {idx}\")\n",
    "ax[1].set_title(f\"Ground Truth Mask {idx}\")\n",
    "ax[2].set_title(f\"Predicted Mask {idx}\")\n",
    "ax[3].set_title(f\"Thresholded Predicted Mask {idx}\")\n",
    "ax[0].axis(\"off\")\n",
    "ax[1].axis(\"off\")\n",
    "ax[2].axis(\"off\")\n",
    "ax[3].axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure Inference Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timing utility\n",
    "import keras\n",
    "from timeit import default_timer as timer\n",
    "# Configurations\n",
    "Total_time = 0.0\n",
    "N_steps = 1000\n",
    "# Load Model\n",
    "model = None\n",
    "model_path = 'Results/Stone/UNetPP_from_scratch_DS/Fold_1/UNetPP_from_scratch_DS_512_Fold_1.keras'\n",
    "model = keras.saving.load_model(model_path)\n",
    "trainable_params = np.sum([np.prod(v.shape) for v in model.trainable_weights])\n",
    "non_trainable_params = np.sum([np.prod(v.shape) for v in model.non_trainable_weights])\n",
    "total_params = trainable_params + non_trainable_params\n",
    "print(f'Model name: UNetPP_from_Scratch')\n",
    "print(f'Trainable Params: {trainable_params}')\n",
    "print(f'Non-trainable Params: {non_trainable_params}')\n",
    "print(f'Total Params: {total_params}\\n')\n",
    "# preprocessing. mean and std from ImageNet\n",
    "# Load Sample Image as Input Data\n",
    "image_path = 'Data/Train/fold_1/No Kidney/wd0036_ps_image_133.png'\n",
    "image = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "tensor = preprocess(image)  # convert image to tensor\n",
    "tensor = tensor.unsqueeze(0)  # reshape 4D tensor (N, C, H, W)\n",
    "tensor = tensor.to(device='cuda')\n",
    "print(tensor.size())\n",
    "for i in range(N_steps):\n",
    "    input_time = timer()\n",
    "    out = model(tensor) \n",
    "    output_time = timer() \n",
    "    output_time = output_time - input_time\n",
    "    Total_time = Total_time + output_time\n",
    "    del out\n",
    "print(f'Total Inference Time: {((Total_time*1000)/N_steps):.4} ms')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PmM2DXYKfN5L"
   },
   "source": [
    "# Keep Session Running Indefinitely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "executionInfo": {
     "elapsed": 358311,
     "status": "error",
     "timestamp": 1663559949014,
     "user": {
      "displayName": "Sakib Mahmud",
      "userId": "13343016238024568099"
     },
     "user_tz": -180
    },
    "id": "HsZa5Ww4fOG_",
    "outputId": "107b288b-8dc4-453d-b2c4-984ba5a99b0f"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "cMJpiCRRB1oh",
    "1j-9ihhfC8xv",
    "sRdWB241j0RU",
    "QtI0XX-ZB1ok",
    "KjYB_2UsB1om",
    "lC-4-y6IB1oz",
    "7qa8CQdxB1o0",
    "p9y1zqdGB1o3"
   ],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wsl_miniconda_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
